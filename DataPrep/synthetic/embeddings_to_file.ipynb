{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f3d435",
   "metadata": {},
   "source": [
    "AUDIO EMBEDDINGS (VGGish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97910c3",
   "metadata": {},
   "source": [
    "Audio No preprocessing - Save to NPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5abaf41-b19e-4363-9f03-549da0e4b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvggish import vggish, vggish_input\n",
    "from tqdm import tqdm  # Optional: for progress bar\n",
    "\n",
    "# Initialize VGGish model\n",
    "audio_model = vggish()\n",
    "audio_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Your original function, unmodified\n",
    "def generateEmbeddingsVGGish(path):\n",
    "    example = vggish_input.wavfile_to_examples(path)\n",
    "    embeddings = audio_model.forward(example)\n",
    "    return embeddings.detach().numpy().reshape(-1)   # Flatten the 0.96 second embeddings into 1\n",
    "\n",
    "# Paths\n",
    "audio_folder = 'data/synthetic_audio'\n",
    "output_file = 'vggish_embeddings.npz'\n",
    "\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "label_to_index = {}\n",
    "\n",
    "# Loop through class subfolders\n",
    "for idx, class_name in enumerate(sorted(os.listdir(audio_folder))):\n",
    "    class_path = os.path.join(audio_folder, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    label_to_index[class_name] = idx\n",
    "\n",
    "    for file_name in tqdm(os.listdir(class_path), desc=f\"Processing {class_name}\"):\n",
    "        if not file_name.endswith('.wav'):\n",
    "            continue\n",
    "        file_path = os.path.join(class_path, file_name)\n",
    "        try:\n",
    "            emb = generateEmbeddingsVGGish(file_path)  # shape [128 * num_chunks]\n",
    "            all_embeddings.append(emb)\n",
    "            all_labels.append(idx-1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Save to .npz\n",
    "np.savez(output_file, embeddings=all_embeddings, labels=all_labels, label_map=label_to_index)\n",
    "\n",
    "print(f\"Saved {len(all_embeddings)} embeddings to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b560903",
   "metadata": {},
   "source": [
    "Audio Zero-Meaned - save to NPZ and plot tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# === Load original audio embeddings ===\n",
    "data = np.load(\"vggish_embeddings.npz\")\n",
    "embeddings = data['embeddings']\n",
    "labels = data['labels']\n",
    "\n",
    "# === Zero-mean the data ===\n",
    "scaler = StandardScaler()\n",
    "embeddings_zm = scaler.fit_transform(embeddings)\n",
    "\n",
    "# === Save zero-meaned embeddings ===\n",
    "np.savez(\"vggish_embeddings-zm.npz\", embeddings=embeddings_zm, labels=labels)\n",
    "print(\"Saved zero-meaned embeddings to vggish_embeddings-zm.npz\")\n",
    "\n",
    "# === t-SNE for both ===\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "\n",
    "tsne_original = tsne.fit_transform(embeddings)\n",
    "tsne_zm = tsne.fit_transform(embeddings_zm)\n",
    "\n",
    "# === Plot side-by-side ===\n",
    "cmap = plt.get_cmap('tab10')\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Original\n",
    "plt.subplot(1, 2, 1)\n",
    "for label in unique_labels:\n",
    "    idx = labels == label\n",
    "    plt.scatter(tsne_original[idx, 0], tsne_original[idx, 1],\n",
    "                color=cmap(label % 10), label=f\"Class {label}\", alpha=0.6, s=40)\n",
    "plt.title(\"Original Audio Embeddings (No Zero-Mean)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Zero-Meaned\n",
    "plt.subplot(1, 2, 2)\n",
    "for label in unique_labels:\n",
    "    idx = labels == label\n",
    "    plt.scatter(tsne_zm[idx, 0], tsne_zm[idx, 1],\n",
    "                color=cmap(label % 10), label=f\"Class {label}\", alpha=0.6, s=40)\n",
    "plt.title(\"Zero-Meaned Audio Embeddings\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"tsne_audio_embeddings_comparison.png\")\n",
    "plt.show()\n",
    "print(\"t-SNE comparison plot saved to tsne_audio_embeddings_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0aae9",
   "metadata": {},
   "source": [
    "IMAGE EMBEDDINGS (VGG16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c105c1a",
   "metadata": {},
   "source": [
    "Image No preprocessing - save to NPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b274d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms as imageT\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "image_folder = 'data/synthetic_shapes'\n",
    "output_file = 'vgg_image_embeddings.npz'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_classes = 6  # Adjust if needed\n",
    "image_ft_size = 128  # Your custom output feature size\n",
    "\n",
    "# ========== LOAD VGG IMAGE MODEL ==========\n",
    "image_model = models.vgg16(pretrained=False)\n",
    "image_model.classifier[n_classes] = nn.Linear(4096, image_ft_size)\n",
    "image_model.load_state_dict(torch.load(\"synthetic-shapes-model.pth\", map_location=device))\n",
    "image_model.to(device)\n",
    "image_model.eval()\n",
    "print(\"VGG image model: loaded successfully\\n\")\n",
    "\n",
    "# ========== IMAGE EMBEDDING FUNCTION ==========\n",
    "def generateEmbeddingsVGG(image_path):\n",
    "    transform = imageT.Compose([\n",
    "        imageT.Resize((224, 224)),\n",
    "        imageT.ToTensor(),\n",
    "        imageT.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img = transform(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        return image_model(img).cpu().numpy().reshape(-1)\n",
    "\n",
    "# ========== PROCESS IMAGES ==========\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "label_to_index = {}\n",
    "\n",
    "for idx, class_name in enumerate(sorted(os.listdir(image_folder))):\n",
    "    class_path = os.path.join(image_folder, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    label_to_index[class_name] = idx\n",
    "\n",
    "    for file_name in tqdm(os.listdir(class_path), desc=f\"Processing {class_name}\"):\n",
    "        if not file_name.endswith('.png'):\n",
    "            continue\n",
    "        file_path = os.path.join(class_path, file_name)\n",
    "        try:\n",
    "            emb = generateEmbeddingsVGG(file_path)\n",
    "            all_embeddings.append(emb)\n",
    "            all_labels.append(idx-1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Save to .npz\n",
    "np.savez(output_file, embeddings=all_embeddings, labels=all_labels, label_map=label_to_index)\n",
    "\n",
    "print(f\"Saved {len(all_embeddings)} image embeddings to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9fb86",
   "metadata": {},
   "source": [
    "Image Zero-Meaned - save to NPZ and plot tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd026982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# === Load original embeddings ===\n",
    "data = np.load(\"vgg_image_embeddings.npz\")\n",
    "embeddings = data['embeddings']\n",
    "labels = data['labels']\n",
    "\n",
    "# === Zero-mean the data ===\n",
    "scaler = StandardScaler()\n",
    "embeddings_zm = scaler.fit_transform(embeddings)\n",
    "\n",
    "# === Save zero-meaned embeddings ===\n",
    "np.savez(\"vgg_image_embeddings-zm.npz\", embeddings=embeddings_zm, labels=labels)\n",
    "print(\"Saved zero-meaned embeddings to vgg_image_embeddings-zm.npz\")\n",
    "\n",
    "# === t-SNE for both ===\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "\n",
    "tsne_original = tsne.fit_transform(embeddings)\n",
    "tsne_zm = tsne.fit_transform(embeddings_zm)\n",
    "\n",
    "# === Plot side-by-side ===\n",
    "cmap = plt.get_cmap('tab10')\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Original\n",
    "plt.subplot(1, 2, 1)\n",
    "for label in unique_labels:\n",
    "    idx = labels == label\n",
    "    plt.scatter(tsne_original[idx, 0], tsne_original[idx, 1],\n",
    "                color=cmap(label % 10), label=f\"Class {label}\", alpha=0.6, s=40)\n",
    "plt.title(\"Original Embeddings (No Zero-Mean)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Zero-Meaned\n",
    "plt.subplot(1, 2, 2)\n",
    "for label in unique_labels:\n",
    "    idx = labels == label\n",
    "    plt.scatter(tsne_zm[idx, 0], tsne_zm[idx, 1],\n",
    "                color=cmap(label % 10), label=f\"Class {label}\", alpha=0.6, s=40)\n",
    "plt.title(\"Zero-Meaned Embeddings\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"tsne_image_embeddings_comparison.png\")\n",
    "plt.show()\n",
    "print(\"t-SNE comparison plot saved to tsne_image_embeddings_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653ff0d",
   "metadata": {},
   "source": [
    "Message consistency within class experiment - save dual labels to NPZ (ex. frequency bucket 1, frequency bucket 2, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3e18f",
   "metadata": {},
   "source": [
    "Dual-label audio: Frequency, No preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfde79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvggish import vggish, vggish_input\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize VGGish model\n",
    "audio_model = vggish()\n",
    "audio_model.eval()\n",
    "\n",
    "def generateEmbeddingsVGGish(path):\n",
    "    example = vggish_input.wavfile_to_examples(path)\n",
    "    embeddings = audio_model.forward(example)\n",
    "    return embeddings.detach().numpy().reshape(-1)\n",
    "\n",
    "# Paths\n",
    "audio_folder = 'data/output_dataset'\n",
    "output_file = 'vggish_dual_labels_embeddings.npz'\n",
    "\n",
    "all_embeddings = []\n",
    "shape_labels = []\n",
    "freq_labels = []\n",
    "\n",
    "shape_label_map = {}\n",
    "freq_label_map = {'0': 0, '1': 1, '2': 2}  # Frequency classes are already 0,1,2\n",
    "\n",
    "# Loop through shape classes\n",
    "for shape_idx, shape_class in enumerate(sorted(os.listdir(audio_folder))):\n",
    "    shape_path = os.path.join(audio_folder, shape_class)\n",
    "    if not os.path.isdir(shape_path):\n",
    "        continue\n",
    "\n",
    "    shape_label_map[shape_class] = shape_idx\n",
    "\n",
    "    for freq_class in sorted(os.listdir(shape_path)):\n",
    "        freq_path = os.path.join(shape_path, freq_class)\n",
    "        if not os.path.isdir(freq_path):\n",
    "            continue\n",
    "\n",
    "        for file_name in tqdm(os.listdir(freq_path), desc=f\"{shape_class}/{freq_class}\"):\n",
    "            if not file_name.endswith('.wav'):\n",
    "                continue\n",
    "            file_path = os.path.join(freq_path, file_name)\n",
    "            try:\n",
    "                emb = generateEmbeddingsVGGish(file_path)\n",
    "                all_embeddings.append(emb)\n",
    "                shape_labels.append(shape_idx)\n",
    "                freq_labels.append(freq_label_map[freq_class])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "shape_labels = np.array(shape_labels)\n",
    "freq_labels = np.array(freq_labels)\n",
    "\n",
    "# Save everything\n",
    "np.savez(output_file, \n",
    "         embeddings=all_embeddings, \n",
    "         shape_labels=shape_labels, \n",
    "         freq_labels=freq_labels, \n",
    "         shape_label_map=shape_label_map, \n",
    "         freq_label_map=freq_label_map)\n",
    "\n",
    "print(f\"Saved {len(all_embeddings)} embeddings with dual labels to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028295e1",
   "metadata": {},
   "source": [
    "Dual-label audio: Frequency, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae061088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load original embeddings\n",
    "data = np.load('vggish_dual_labels_embeddings.npz', allow_pickle=True)\n",
    "\n",
    "embeddings = data['embeddings']\n",
    "shape_labels = data['shape_labels']\n",
    "freq_labels = data['freq_labels']\n",
    "shape_label_map = data['shape_label_map'].item()\n",
    "freq_label_map = data['freq_label_map'].item()\n",
    "\n",
    "print(f\"Original embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# Apply PCA to reduce to 128 dimensions\n",
    "pca = PCA(n_components=128)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "print(f\"Reduced embedding shape: {reduced_embeddings.shape}\")\n",
    "\n",
    "# Save to new .npz file\n",
    "np.savez('vggish_dual_labels_embeddings-pca.npz',\n",
    "         embeddings=reduced_embeddings,\n",
    "         shape_labels=shape_labels,\n",
    "         freq_labels=freq_labels,\n",
    "         shape_label_map=shape_label_map,\n",
    "         freq_label_map=freq_label_map)\n",
    "\n",
    "print(\"Saved PCA-reduced embeddings to 'vggish_dual_labels_embeddings-pca.npz'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f472b",
   "metadata": {},
   "source": [
    "Dual-label audio: Amplitude, No preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214aab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvggish import vggish, vggish_input\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize VGGish model\n",
    "audio_model = vggish()\n",
    "audio_model.eval()\n",
    "\n",
    "def generateEmbeddingsVGGish(path):\n",
    "    example = vggish_input.wavfile_to_examples(path)\n",
    "    embeddings = audio_model.forward(example)\n",
    "    return embeddings.detach().numpy().reshape(-1)\n",
    "\n",
    "# Paths\n",
    "audio_folder = 'data/output_dataset_amplitude'\n",
    "output_file = 'vggish_dual_labels_embeddings_amp.npz'\n",
    "\n",
    "all_embeddings = []\n",
    "shape_labels = []\n",
    "freq_labels = []\n",
    "\n",
    "shape_label_map = {}\n",
    "freq_label_map = {'0': 0, '1': 1, '2': 2}  # Frequency classes are already 0,1,2\n",
    "\n",
    "# Loop through shape classes\n",
    "for shape_idx, shape_class in enumerate(sorted(os.listdir(audio_folder))):\n",
    "    shape_path = os.path.join(audio_folder, shape_class)\n",
    "    if not os.path.isdir(shape_path):\n",
    "        continue\n",
    "\n",
    "    shape_label_map[shape_class] = shape_idx\n",
    "\n",
    "    for freq_class in sorted(os.listdir(shape_path)):\n",
    "        freq_path = os.path.join(shape_path, freq_class)\n",
    "        if not os.path.isdir(freq_path):\n",
    "            continue\n",
    "\n",
    "        for file_name in tqdm(os.listdir(freq_path), desc=f\"{shape_class}/{freq_class}\"):\n",
    "            if not file_name.endswith('.wav'):\n",
    "                continue\n",
    "            file_path = os.path.join(freq_path, file_name)\n",
    "            try:\n",
    "                emb = generateEmbeddingsVGGish(file_path)\n",
    "                all_embeddings.append(emb)\n",
    "                shape_labels.append(shape_idx)\n",
    "                freq_labels.append(freq_label_map[freq_class])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "shape_labels = np.array(shape_labels)\n",
    "freq_labels = np.array(freq_labels)\n",
    "\n",
    "# Save everything\n",
    "np.savez(output_file, \n",
    "         embeddings=all_embeddings, \n",
    "         shape_labels=shape_labels, \n",
    "         freq_labels=freq_labels, \n",
    "         shape_label_map=shape_label_map, \n",
    "         freq_label_map=freq_label_map)\n",
    "\n",
    "print(f\"Saved {len(all_embeddings)} embeddings with dual labels to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4252fa0",
   "metadata": {},
   "source": [
    "Dual-label audio: Amplitude, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load original embeddings\n",
    "data = np.load('vggish_dual_labels_embeddings_amp.npz', allow_pickle=True)\n",
    "\n",
    "embeddings = data['embeddings']\n",
    "shape_labels = data['shape_labels']\n",
    "freq_labels = data['freq_labels']\n",
    "shape_label_map = data['shape_label_map'].item()\n",
    "freq_label_map = data['freq_label_map'].item()\n",
    "\n",
    "print(f\"Original embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# Apply PCA to reduce to 128 dimensions\n",
    "pca = PCA(n_components=128)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "print(f\"Reduced embedding shape: {reduced_embeddings.shape}\")\n",
    "\n",
    "# Save to new .npz file\n",
    "np.savez('vggish_dual_labels_embeddings_amp-pca.npz',\n",
    "         embeddings=reduced_embeddings,\n",
    "         shape_labels=shape_labels,\n",
    "         freq_labels=freq_labels,\n",
    "         shape_label_map=shape_label_map,\n",
    "         freq_label_map=freq_label_map)\n",
    "\n",
    "print(\"Saved PCA-reduced embeddings to 'vggish_dual_labels_embeddings_amp-pca.npz'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg36",
   "language": "python",
   "name": "egg36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
