{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0221eb3-f92e-4e70-8c8d-33481ef5e077",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'archs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#------------------------------------------Imports------------------------------------------#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Basic Model and Training Stuff\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marchs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sender, Receiver, Baseline \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'archs'"
     ]
    }
   ],
   "source": [
    "#------------------------------------------Imports------------------------------------------#\n",
    "\n",
    "# Basic Model and Training Stuff\n",
    "from archs import Sender, Receiver, Baseline \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset,random_split\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "from typing import Dict, Any, Tuple, List\n",
    "from torch import Tensor\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Evaluation\n",
    "#from evaluation import track_conversation_length, convmean_per_class, lengths_over_time\n",
    "\n",
    "# System Measurement - Time, RAM, CPU etc.\n",
    "import psutil\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "#from plot_metrics import plot_accuracy, plot_loss, plot_lr_vs_loss, plot_epoch_times, plot_system_usage, plot_entropy\n",
    "\n",
    "#Data Libraries\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torchaudio\n",
    "import torchaudio.transforms as audioT\n",
    "import torchvision.transforms as imageT\n",
    "\n",
    "# Embedding Generator Models\n",
    "from torchvggish import vggish, vggish_input\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------Parameter Definition------------------------------------------#\n",
    "device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \"+str(device)+\"\\n\")\n",
    "conv_lengths_epochs = []\n",
    "runfile = '../Models/unimodal/'\n",
    "\n",
    "#-------------Game Settings--------------#\n",
    "unimodal = True\n",
    "audio = True # if audio is sender\n",
    "dynamic = True\n",
    "sender_is_learning = True # Funny that this actually came from a bug\n",
    "\n",
    "n_classes = 6\n",
    "n_distractors = 2\n",
    "message_size = 10\n",
    "max_conv_length=10\n",
    "audio_ft_size = 128 # Changeable: image=128, audio=whateve\n",
    "image_ft_size = 128 \n",
    "fixed_exchange = False\n",
    "batch_size=1\n",
    "\n",
    "sender_hidden_size = 128\n",
    "receiver_hidden_size = 128\n",
    "\n",
    "# Instantiating Models. If audio is sender, sender audio. If unimodal etc\n",
    "if audio:\n",
    "    sender = Sender(feat_dim=audio_ft_size, h_dim=sender_hidden_size, w_dim=message_size, bin_dim_out=message_size, use_binary=True).to(device)\n",
    "    if unimodal:\n",
    "        receiver = Receiver(z_dim=message_size, desc_dim=audio_ft_size, hid_dim=receiver_hidden_size, out_dim=1, w_dim=message_size, s_dim=1, use_binary=True).to(device)\n",
    "    else:\n",
    "        receiver = Receiver(z_dim=message_size, desc_dim=image_ft_size, hid_dim=receiver_hidden_size, out_dim=1, w_dim=message_size, s_dim=1, use_binary=True).to(device)\n",
    "        \n",
    "else:\n",
    "    sender = Sender(feat_dim=image_ft_size, h_dim=sender_hidden_size, w_dim=message_size, bin_dim_out=message_size, use_binary=True).to(device)\n",
    "    if unimodal:\n",
    "        receiver = Receiver(z_dim=message_size, desc_dim=image_ft_size, hid_dim=receiver_hidden_size, out_dim=1, w_dim=message_size, s_dim=1, use_binary=True).to(device)\n",
    "    else: \n",
    "        receiver = Receiver(z_dim=message_size, desc_dim=audio_ft_size, hid_dim=receiver_hidden_size, out_dim=1, w_dim=message_size, s_dim=1, use_binary=True).to(device)\n",
    "\n",
    "baseline_sen = Baseline(hid_dim=128, x_dim=128, binary_dim=message_size, inp_dim=0).to(device)\n",
    "baseline_rec = Baseline(hid_dim=128, x_dim=128, binary_dim=message_size, inp_dim=0).to(device)\n",
    "\n",
    "print(f\"Game settings:\\nDistractors={n_distractors}   Classes={n_classes}  MessageSize={message_size}   AudioSize={audio_ft_size}   ImageSize={image_ft_size}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------- Load up models and data ----------------------â€“#\n",
    "# How to load them up later:\n",
    "checkpoint = torch.load(runfile+'models_checkpoint.pth')\n",
    "\n",
    "sender.load_state_dict(checkpoint['sender_state_dict'])\n",
    "receiver.load_state_dict(checkpoint['receiver_state_dict'])\n",
    "baseline_sen.load_state_dict(checkpoint['baseline_sen_state_dict'])\n",
    "baseline_rec.load_state_dict(checkpoint['baseline_rec_state_dict'])\n",
    "sender.eval()\n",
    "receiver.eval()\n",
    "\n",
    "\n",
    "class SyntheticData(Dataset):\n",
    "    def __init__(self, n_distractors,audio_embedding_file,image_embedding_file,transform=None):\n",
    "        \"\"\"\n",
    "        - audio_root_dir: Path to `synthetic_audio/`\n",
    "        - image_root_dir: Path to `synthetic_shapes/`\n",
    "        - transform: Any optional transformations (not used here)\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.n_distractors = n_distractors\n",
    "\n",
    "        if audio:\n",
    "            # Load audio data\n",
    "            data = np.load(audio_embedding_file)\n",
    "            dis_data = np.load(audio_embedding_file if unimodal else image_embedding_file)\n",
    "        else:\n",
    "            #Load images\n",
    "            data = np.load(image_embedding_file)\n",
    "            dis_data = np.load(image_embedding_file if unimodal else audio_embedding_file)\n",
    "\n",
    "\n",
    "        #Sender's input\n",
    "        self.embeddings = data['embeddings']\n",
    "        self.labels = data['labels']\n",
    "\n",
    "        # Receiver's input\n",
    "        self.distembeddings = dis_data['embeddings']\n",
    "        self.distlabels = dis_data['labels']\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - Embedding (Tensor) of shape [N * 128]\n",
    "        - Class label (int)\n",
    "        \"\"\"\n",
    "        target_embedding = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        dist_embeddings = []\n",
    "        if not dynamic:\n",
    "            dist_embeddings = emb_set\n",
    "        else:\n",
    "            for i in range(0,6):\n",
    "                idx = np.where(self.distlabels == i)[0]\n",
    "                random_index = np.random.choice(idx)\n",
    "                emb = torch.tensor(self.distembeddings[random_index], dtype=torch.float32)\n",
    "                dist_embeddings.append(emb)\n",
    "            dist_embeddings = torch.stack(dist_embeddings, dim=0).squeeze(1)\n",
    "            \n",
    "            \n",
    "        correct_image_index = label\n",
    "        \n",
    "        \n",
    "        return target_embedding.to(device), dist_embeddings.to(device), torch.tensor(label, dtype=torch.long).to(device), torch.tensor(correct_image_index, dtype=torch.long).to(device)\n",
    "# Loading it up later\n",
    "with open(runfile+'test_dataset.pkl', 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------Conversation Function------------------------------------------#\n",
    "def conversation(sender, receiver, baseline_sen, baseline_rec, exchange_args: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Handles a communication exchange between a sender and a receiver.\n",
    "    The sender encodes an audio feature into a message, which is interpreted by the receiver.\n",
    "    \n",
    "    Parameters:\n",
    "        sender: Sender model that generates communication signals.\n",
    "        receiver: Receiver model that interprets messages from the sender.\n",
    "        baseline_sen: Baseline sender model used for scoring.\n",
    "        baseline_rec: Baseline receiver model used for scoring.\n",
    "        exchange_args: Dictionary containing all input arguments.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing conversation details including stop conditions, messages, predictions, and losses.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract conversation parameters\n",
    "    audio_feats = exchange_args[\"audio\"]  # Audio features input\n",
    "    target = exchange_args[\"target\"]  # Class labels\n",
    "    distractors = exchange_args[\"distractors\"]  # Set of distractors\n",
    "    train = exchange_args[\"train\"]  # True for training, False for evaluation\n",
    "    break_early = exchange_args.get(\"break_early\", False)  # Whether to terminate early\n",
    "\n",
    "    batch_size = audio_feats.size(0) # If one audio clip, then batch size = 1\n",
    "    first_rec = torch.zeros(sender.w_dim).to(device)  # Placeholder for first message initialization (starts at 00000... etc)\n",
    "\n",
    "    # Initialize communication tracking variables\n",
    "    stop_mask = [torch.ones(batch_size, 1, dtype=torch.uint8)]  # Mask for stopping conversation\n",
    "    stop_feat, stop_prob = [], []  # Stop features and probabilities\n",
    "    sen_feats, sen_probs = [], []  # Sender messages and probabilities\n",
    "    rec_feats, rec_probs = [], []  # Receiver messages and probabilities\n",
    "    y, bs, br = [], [], []  # Predictions, sender loss, receiver loss\n",
    "\n",
    "    # Define the initial binary message\n",
    "    w_binary = first_rec.expand(batch_size, sender.w_dim).clone().to(device) # Cloning this into a diff memory address\n",
    "\n",
    "    # Set training mode\n",
    "    if train:\n",
    "        sender.train()\n",
    "        receiver.train()\n",
    "        baseline_sen.train()\n",
    "        baseline_rec.train()\n",
    "    else:\n",
    "        sender.eval()\n",
    "        receiver.eval()\n",
    "\n",
    "    receiver.reset_state()  # Reset receiver state before communication begins\n",
    "\n",
    "    max_exchange = max_conv_length  # Define maximum message exchanges to prevent infinite loops\n",
    "\n",
    "    #---------Conversation Loop----------#\n",
    "    #print(\"--------------------Conversation-------------------\")\n",
    "    for i_exch in range(max_exchange):\n",
    "        z_r = w_binary.to(device)  # Receiver's message\n",
    "        #print(\"Receiver: \"+str(z_r))\n",
    "        \n",
    "        # Sender processes audio features and previous message to generate new communication\n",
    "        with torch.no_grad() if not train else torch.enable_grad():\n",
    "            z_binary, z_probs = sender(audio_feats, z_r)\n",
    "\n",
    "    \n",
    "        z_s = z_binary  # Sender's message to be received\n",
    "        \n",
    "        # Receiver interprets the sender's message\n",
    "        with torch.no_grad() if not train else torch.enable_grad():\n",
    "            (s_binary, s_prob), (w_binary, w_probs), outp = receiver(z_s, exchange_args[\"distractors\"])\n",
    "        \n",
    "        # Compute baseline scores if training\n",
    "        if train:\n",
    "            sen_h_x: Tensor = sender.h_x.to(device) # Sender hidden states\n",
    "            with torch.no_grad():\n",
    "                baseline_sen_scores = baseline_sen(sen_h_x, z_r, None) # Estimates loss using senders internal state + receiver's message\n",
    "\n",
    "            rec_h_z: Tensor = receiver.h_z if receiver.h_z.to(device) is not None else receiver.initial_state(batch_size).to(device) #Receiver's hidden state (if None initializes new state)\n",
    "            with torch.no_grad():\n",
    "                #print(\"Message size: \"+str(z_s.shape) + \" Hidden State size: \"+str(rec_h_z.shape))\n",
    "                baseline_rec_scores = baseline_rec(None, z_s, rec_h_z) #Estimates scores using sender's msg and receiver's hidden state\n",
    "\n",
    "        # Compute log probabilities and determine predictions\n",
    "        outp = outp.view(batch_size, -1)\n",
    "        dist = F.log_softmax(outp, dim=1)\n",
    "        maxdist, argmax = dist.max(dim=1) # Model's final prediction, never used?\n",
    "\n",
    "        # Store conversation history\n",
    "        stop_mask.append(torch.min(stop_mask[-1], s_binary.byte()))\n",
    "        stop_feat.append(s_binary)\n",
    "        stop_prob.append(s_prob)\n",
    "        sen_feats.append(z_binary)\n",
    "        sen_probs.append(z_probs)\n",
    "        rec_feats.append(w_binary)\n",
    "        rec_probs.append(w_probs)\n",
    "        y.append(outp)\n",
    "\n",
    "        if train:\n",
    "            br.append(baseline_rec_scores)\n",
    "            bs.append(baseline_sen_scores)\n",
    "\n",
    "        # Terminate exchange if all conversations are complete\n",
    "        if break_early and stop_mask[-1].float().sum().item() == 0:\n",
    "            break\n",
    "\n",
    "    # Ensure final stop mask is zero\n",
    "    stop_mask[-1].fill_(0)\n",
    "    \n",
    "    # Return results of conversation\n",
    "    s = (stop_mask, stop_feat, stop_prob)\n",
    "    sen_w = (sen_feats, sen_probs)\n",
    "    rec_w = (rec_feats, rec_probs)\n",
    "    #print(\"------------------Conversation Done------------------\")\n",
    "    #print(y)\n",
    "    \n",
    "    return s, sen_w, rec_w, y, bs, br\n",
    "\n",
    "\n",
    "\n",
    "def entropy_per_conv(samples=8):\n",
    "    num_batches = len(test_loader)\n",
    "    selected_indices = set(random.sample(range(num_batches), samples))\n",
    "\n",
    "    entropy_per = []\n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        if idx in selected_indices:\n",
    "            #print(f\"Batch {idx}: Selected for special processing\")\n",
    "            audio, distractors, target, correct_index = batch\n",
    "            exchange_args = {\n",
    "                    \"audio\": audio,  \n",
    "                    \"target\": target,\n",
    "                    \"distractors\": distractors,\n",
    "                    \"desc\": None,  \n",
    "                    \"train\": False,  # Set to False for testing\n",
    "                    \"break_early\": False\n",
    "            }\n",
    "\n",
    "            s, sen_w, rec_w, y, bs, br = conversation(sender, receiver, baseline_sen, baseline_rec, exchange_args)\n",
    "            s_masks, s_feats, s_probs = s\n",
    "                \n",
    "            # for i, tensor in enumerate(s_feats):\n",
    "            # # Assuming each tensor is at least of size [1] or more\n",
    "            #     first_value = tensor[0]\n",
    "            #     print(f\"Tensor {i} - First value: {first_value}\")\n",
    "            \n",
    "            sen_feats, sen_probs = sen_w\n",
    "            rec_feats, rec_probs = rec_w\n",
    "        \n",
    "            # Mask loss if dynamic exchange length\n",
    "            binary_s_masks = binary_rec_masks = binary_sen_masks = None\n",
    "            bas_rec_masks = bas_sen_masks = None\n",
    "            y_masks = None\n",
    "\n",
    "            classification_entropy = []\n",
    "            for timestep in y:\n",
    "                dist = F.log_softmax(timestep, dim=1)\n",
    "                entropy_vals = entropy_from_log_probs(dist)\n",
    "\n",
    "                entropy_scalar = entropy_vals[0].squeeze().item()\n",
    "                classification_entropy.append(entropy_scalar)\n",
    "        \n",
    "            entropy_per.append(classification_entropy)\n",
    "\n",
    "            timesteps = list(range(1, 11))  # Assuming 10 timesteps\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for i, entropy_list in enumerate(entropy_per):\n",
    "                plt.plot(timesteps, entropy_list, label=f'Sample {i+1}')\n",
    "            \n",
    "            plt.xlabel('Timestep')\n",
    "            plt.ylabel('Classification Entropy')\n",
    "            plt.title('Entropy per Conversation Over Time')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('entropy_per_conversation_timestep.png')\n",
    "            plt.close()\n",
    "            print(\"Saved plot as entropy_per_conversation.png\")\n",
    "        \n",
    "def get_rec_outp(y, masks):\n",
    "    def negent(yy):\n",
    "        probs = F.softmax(yy)\n",
    "        return (torch.log(probs + 1e-8) * probs).sum(1).mean()\n",
    "\n",
    "    negentropy = map(negent, y)\n",
    "\n",
    "    if masks is not None:\n",
    "\n",
    "        batch_size = y[0].size(0)\n",
    "        exchange_steps = len(masks)\n",
    "\n",
    "        inp = torch.cat([yy.view(batch_size, 1, -1) for yy in y], 1)\n",
    "        mask = torch.cat(masks, 1).view(\n",
    "            batch_size, exchange_steps, 1).expand_as(inp)\n",
    "        outp = torch.masked_select(inp, mask.detach().bool()).view(batch_size, -1)\n",
    "        return outp, negentropy\n",
    "    else:\n",
    "        return y[-1], negentropy\n",
    "\n",
    "\n",
    "def entropy_from_log_probs(log_probs):\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(log_probs * probs).sum(dim=1)\n",
    "    return [e.unsqueeze(0) for e in entropy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1a31a-9bd2-43df-8e25-3a05c26e3b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
